---
name: crawl-korea-live
description: "Research Korean live commerce platforms (SSG, Naver, Kakao, CJ OnStyle) and build crawlers to extract products, events, coupons, benefits, and promotions"
metadata:
  category: project-specific
  argument-hint: "[platform-name]"
---

# Korea Live Commerce Crawler Research & Development

Research Korean live commerce platforms and build crawlers following the project architecture.

## IMPORTANT: Brand Filtering & Multi-Brand Detection

**Only save broadcasts for brands that exist in the `brands` table.**

- Before saving a broadcast, check if the brand exists in the database
- If brand is not found in DB â†’ **SKIP the broadcast** (do not save)
- Log skipped broadcasts with reason: "Brand not in DB: {brand_name}"

**Multi-brand detection** is handled automatically by upserters (`crawler/shared/brand_utils.py`):
- Detects all brands in title AND product names against known brand keywords
- Classifies as **single** ("ì„¤í™”ìˆ˜"), **collab** ("IOPE X Primera"), or **union** ("ì—ìŠ¤íŠ¸ë¼ ì™¸")
- Sets `brand_display` (display string) and `brand_data` (JSONB classification) on the broadcast
- Resolves `brand_ids` map for all detected brands
- The primary `brand_name` / `brand_id` FK remains the single main brand for backward compatibility
- New platform crawlers get this for free â€” the shared upserter handles it after brand lookup

## Target Platforms

| Platform | URL | Status | DB Count |
|----------|-----|--------|----------|
| Naver Shopping Live | shoppinglive.naver.com | Active | 466 |
| Kakao Shopping Live | shoppinglive.kakao.com | Active | 36 |
| Amoremall | amoremall.com | Active | 20 |
| SSG Live | m.ssg.com/liveCommerce | Active | 10 |
| 11Street Live | 11st.co.kr | Active | 10 |
| CJ OnStyle | cjonestyle.com | Scaffold | 0 |
| GS Shop Live | gsshop.com | Scaffold | 0 |

## Database Schema (2 Tables + Supporting)

### Table 1: `broadcasts` (40 columns)

#### Core Fields

| Column | Type | Description |
|--------|------|-------------|
| `id` | bigint | Primary key |
| `external_id` | text | Platform's broadcast ID (composite unique key with platform_id) |
| `platform_id` | uuid | FK to platforms |
| `title` | text | Broadcast title |
| `description` | text | Broadcast description |
| `brand_name` | text | Primary brand name |
| `brand_id` | uuid | FK to brands (primary brand) |
| `brand_display` | text | Multi-brand display string ("ì„¤í™”ìˆ˜", "IOPE X Primera", "ë¼ë„¤ì¦ˆ ì™¸") |
| `brand_data` | jsonb | Multi-brand classification (brand_type, all_brands, brand_ids, etc.) |
| `status` | text | Broadcast status (see Status Mapping) |
| `broadcast_type` | text | replays/lives/shortclips/scheduled |
| `broadcast_format` | text | solo/collaboration/brand_hall/special (vision-extracted) |

#### URLs

| Column | Type | Description |
|--------|------|-------------|
| `broadcast_url` | text | Live/viewing URL |
| `replay_url` | text | VOD/replay URL |
| `livebridge_url` | text | Detail/promotion page URL |
| `stand_by_image` | text | Thumbnail image URL |

#### Timestamps

| Column | Type | Description |
|--------|------|-------------|
| `broadcast_date` | timestamptz | Actual start time |
| `broadcast_end_date` | timestamptz | End time |
| `expected_start_date` | timestamptz | Scheduled start time |
| `benefit_start_time` | timestamptz | Benefit validity start |
| `benefit_end_time` | timestamptz | Benefit validity end |
| `benefit_application_time` | text | When benefits apply ("êµ¬ë§¤ ì¦‰ì‹œ", "ë¼ì´ë¸Œ ì¤‘") |
| `created_at` | timestamptz | Row created |
| `updated_at` | timestamptz | Row updated |

#### Vision-Extracted JSONB

| Column | Type | Description |
|--------|------|-------------|
| `benefits` | jsonb | Unified benefits [{benefit_category, benefit_title, items}] |
| `special_goods` | jsonb | Vision-extracted products [{name, prices, includes, gift_items}] |
| `participation_events` | jsonb | Events [{event_type, event_name, prize, method}] |
| `precautions` | jsonb | Precautions [{category, content}] |
| `notices` | jsonb | Notice items [{content}] |
| `promotion_images` | jsonb | Array of promotion image URLs |

#### API-Sourced JSONB

| Column | Type | Description |
|--------|------|-------------|
| `announcements` | jsonb | From notice API / notice pages |
| `host_chat` | jsonb | Host messages during broadcast (Naver replays/extras) |
| `chat_messages` | jsonb | Live chat messages |
| `coupons_data` | jsonb | Coupon information |
| `benefits_data` | jsonb | API benefits (separate from vision `benefits`) |
| `purchase_benefits` | jsonb | Purchase-based benefits |
| `qna_data` | jsonb | Q&A format {qna_items, summary} (SSG, Kakao) |
| `comments_data` | jsonb | Unified comments {comments, summary} (Naver livebridge) |
| `chat_summary` | jsonb | Chat statistics |

#### Debug

| Column | Type | Description |
|--------|------|-------------|
| `raw_data` | jsonb | Full raw extraction data (vision + API separated) |

### Table 2: `broadcast_products`

| Column | Type | Description |
|--------|------|-------------|
| `id` | bigint | Primary key |
| `broadcast_id` | bigint | FK to broadcasts |
| `product_id` | text | Platform's product ID (unique with broadcast_id) |
| `name` | text | Product name |
| `brand_name` | text | Product brand |
| `original_price` | numeric | Original price |
| `discounted_price` | numeric | Sale price |
| `discount_rate` | numeric | Discount percentage |
| `stock` | integer | Inventory count |
| `image_url` | text | Product image URL |
| `link_url` | text | Product page URL |
| `review_count` | integer | Review count |
| `delivery_fee` | text | Delivery fee info |
| `product_classification` | text | `live` (API) or `main` (vision-extracted) |
| `capacity` | text | Product volume/capacity |
| `quantity` | text | Quantity info |
| `raw_data` | jsonb | Full original product data |
| `created_at` | timestamptz | Row created |
| `updated_at` | timestamptz | Row updated |

## Existing Crawler Implementations

### Naver Shopping Live (`crawler/cj/`)

**Approach**: Playwright API interception + Vision LLM

**Entry points**:
- `naver_broadcast_crawler.py` â€” Single URL crawl (`python naver_broadcast_crawler.py URL [--save-to-db]`)
- `standalone_crawler.py` â€” Brand-based search + batch crawl
- `run_livebridge_crawler.py` â€” Livebridge detail page crawl with vision
- `save_live11_productgrid_and_insert_db.py` â€” Full production pipeline

**URL types supported**:

| URL Pattern | Crawler | Method |
|-------------|---------|--------|
| `/replays/{id}` | ReplaysCrawler | API interception |
| `/lives/{id}` | LivesCrawler | Hybrid (JSON + API) |
| `/shortclips/{id}` | ShortClipsCrawler | Hybrid (JSON + API) |
| `/livebridge/{id}` | LivebridgeCrawler | Vision LLM + API |

**Data flow**:
```
Playwright page â†’ API interception (broadcast, coupons, benefits, comments, promotions, notice-all)
  â†’ Extraction â†’ Optional livebridge vision â†’ Merge (API + vision with source labels)
  â†’ Transformer (transformer.py) â†’ Upserter (brand lookup + multi-brand + platform lookup)
  â†’ Supabase upsert (broadcasts + broadcast_products)
```

**Key files**:
```
crawler/cj/
â”œâ”€â”€ naver_broadcast_crawler.py    # CLI entry point
â”œâ”€â”€ standalone_crawler.py         # Batch search+crawl
â”œâ”€â”€ run_livebridge_crawler.py     # Livebridge CLI
â”œâ”€â”€ crawler_utils.py              # Benefit/event merging with source labels
â”œâ”€â”€ crawlers/
â”‚   â”œâ”€â”€ base_crawler.py           # Abstract base + livebridge auto-crawl
â”‚   â”œâ”€â”€ replays_crawler.py        # /replays/ handler
â”‚   â”œâ”€â”€ lives_crawler.py          # /lives/ handler
â”‚   â”œâ”€â”€ shortclips_crawler.py     # /shortclips/ handler
â”‚   â””â”€â”€ livebridge_crawler.py     # /livebridge/ vision extraction
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ api_extractor.py          # API response interception
â”‚   â””â”€â”€ json_extractor.py         # __NEXT_DATA__ extraction
â”œâ”€â”€ persistence/
â”‚   â”œâ”€â”€ transformer.py            # Naver â†’ DB schema mapping
â”‚   â”œâ”€â”€ upserter.py               # DB upsert with multi-brand detection
â”‚   â”œâ”€â”€ saver.py                  # High-level save interface
â”‚   â”œâ”€â”€ client.py                 # Supabase client
â”‚   â””â”€â”€ validator.py              # Schema validation
â””â”€â”€ utils/
    â”œâ”€â”€ url_detector.py           # URL type detection
    â”œâ”€â”€ browser_pool.py           # Playwright browser reuse
    â””â”€â”€ checkpoint_manager.py     # Resumable crawl state
```

**Brand detection (Naver)**: Priority chain â€” `products[0].brandName` â†’ `categoryComponent.brandName` â†’ `nickname`

**Product classification**:
- `live`: From Naver product API (standard products)
- `main`: From livebridge vision extraction (special goods with detailed pricing)

---

### SSG Live (`crawler/platforms/ssg/`)

**Approach**: Playwright HTML scraping + Gripcloud API + Vision LLM

**Entry points**:
- `crawler.py` â€” VOD crawl (`python crawler.py --mode amore --vision --save-db --concurrency 5`)
- `schedule_crawler.py` â€” Scheduled broadcast crawl (`python schedule_crawler.py --mode amore --vision --save-db`)

**Data sources**:

| Data | Source | Method |
|------|--------|--------|
| Broadcast list | `m.ssg.com/liveCommerce` | Playwright scroll + DOM |
| Products | `m.ssg.com/liveCommerce/ajaxGetBrocItems.ssg` | REST API |
| Notices | `play.gripcloud.show/player/v1/faq/{ch_id}` | REST API |
| Q&A | `play.gripcloud.show/v1/qna/vod/{vod_id}` | REST API |
| Announcements | `window.param.content.description` | Gripcloud player DOM |
| Detail images | Detail page DOM | Playwright |
| Benefits/events | Detail page images | Vision LLM (GPT-4o-mini) |

**Data flow**:
```
Playwright scrolls VOD list â†’ Filter by tracked brands â†’ Concurrent processing (5):
  Products (API) + Notices (API) + QnA (API) + Detail images (Playwright)
  â†’ Vision extraction â†’ SSGTransformer â†’ Shared DatabaseUpserter (multi-brand)
  â†’ Supabase upsert
```

**Key files**:
```
crawler/platforms/ssg/
â”œâ”€â”€ crawler.py                    # VOD crawler (main)
â”œâ”€â”€ schedule_crawler.py           # Scheduled broadcast crawler
â”œâ”€â”€ config.py                     # URLs, headers, selectors
â”œâ”€â”€ html_selectors.py             # CSS selectors and regex
â”œâ”€â”€ ssg_persistence/
â”‚   â”œâ”€â”€ saver.py                  # SSG-specific save interface
â”‚   â””â”€â”€ transformer.py            # SSG â†’ DB schema mapping
â””â”€â”€ investigation/                # Research scripts (gitignored)
```

**Brand detection (SSG)**: Title-only keyword matching (NOT product brand_name â€” unreliable on SSG)

**Product classification**: Same as Naver â€” `live` (API) and `main` (vision)

**ID generation**: MD5 hash of `ch_xxxxx` â†’ numeric range 100M-2.1B (avoids Naver ID collision)

## JSONB Field Structures (Database Format)

Reference: `crawler/cj/persistence/transformer.py`, `crawler/shared/persistence/base_transformer.py`

### `benefits` (êµ¬ë§¤ í˜œíƒ - vision-extracted)

**benefit_category values (6 UPPERCASE values from vision prompt):**
`GIFT_BY_AMOUNT`, `GIFT_PROMOTION`, `COUPON`, `POINT`, `SERVICE`, `OTHER`

```json
[
  {
    "benefit_category": "GIFT_BY_AMOUNT",
    "benefit_title": "êµ¬ë§¤ê¸ˆì•¡ëŒ€ë³„ ì‚¬ì€í’ˆ",
    "benefit_details": "ìƒì„¸ ì„¤ëª…",
    "items": [
      {"condition": "4ë§Œì› ì´ìƒ", "name": "ì„¸ëŸ¼", "volume": "5ml", "quantity": "1ê°œ", "additional_info": null},
      {"condition": "8ë§Œì› ì´ìƒ", "name": "í…€ë¸”ëŸ¬", "additional_info": "í•œì •ìˆ˜ëŸ‰ 100ê°œ"}
    ],
    "target_scope": "ì „ì›",
    "validity_period": "ë°©ì†¡ì¤‘ë§Œ",
    "additional_info": "í•©ë°°ì†¡, êµ¬ë§¤í™•ì • í•„ìš”"
  }
]
```

### `benefits_data` (API í˜œíƒ - Naver API)

```json
[
  {
    "benefit_id": "12345",
    "benefit_type": "ONAIR",
    "message": "ë¼ì´ë¸Œ í•œì • í˜œíƒ",
    "detail": "2ë§Œì› ì´ìƒ êµ¬ë§¤ì‹œ ì‚¬ì€í’ˆ ì¦ì •",
    "raw_data": {"id": "12345", "type": "ONAIR", "message": "...", "detail": "..."}
  }
]
```

### `special_goods` (ë¼ì´ë¸Œ íŠ¹ê°€ ìƒí’ˆ - vision-extracted)

**includes** = ê¸°ë³¸ êµ¬ì„±í’ˆ, **gift_items** = ì¦ì •í’ˆ(ë¤)

> **Note**: Vision prompt outputs `price_tiers` array, but transformer maps to flat fields in DB.

```json
[
  {
    "name": "ìŠˆí¼ë°”ì´íƒˆ 2ì¢… ì„¸íŠ¸",
    "description": "íŠ¸ë¦¬í”Œ ê¸°íš",
    "original_price": "139,000ì›",
    "first_discount_price": "100,080ì›",
    "first_discount_rate": "28%",
    "max_discount_price": "93,190ì›",
    "max_discount_rate": "33%",
    "includes": ["ìŠˆí¼ë°”ì´íƒˆ í¬ë¦¼ 60ml", "ìŠˆí¼ë°”ì´íƒˆ ì„¸ëŸ¼ 50ml"],
    "tags": ["ë‹¨ë…ìƒí’ˆ", "íŠ¸ë¦¬í”Œ ê¸°íš"],
    "stock_info": "í•œì •ìˆ˜ëŸ‰",
    "gift_items": ["í† íŠ¸ë°±", "5ì¢… í‚¤íŠ¸"],
    "additional_info": "í¬í† ë¦¬ë·° 1,000ì› ì ë¦½"
  }
]
```

Vision prompt `price_tiers` format (pre-transform):
```json
{
  "price_tiers": [
    {"label": "ì •ìƒê°€", "price": "139,000ì›"},
    {"label": "ë¼ì´ë¸Œ íŠ¹ê°€", "price": "100,080ì›", "discount_rate": "28%"},
    {"label": "ìµœëŒ€ í• ì¸ê°€", "price": "93,190ì›", "discount_rate": "33%"}
  ]
}
```

### `participation_events` (ì°¸ì—¬ ì´ë²¤íŠ¸ - vision-extracted)

**event_type values (lowercase - after transformer mapping):**
`purchase_verification`, `purchase_king`, `chat_king`, `photo_review`, `first_come`, `raffle`, `share`, `other`

```json
[
  {
    "event_type": "purchase_verification",
    "event_name": "êµ¬ë§¤ì¸ì¦ ì´ë²¤íŠ¸",
    "prize": "ìŠ¤íƒ€ë²…ìŠ¤ ê¸°í”„í‹°ì½˜",
    "participation_method": "êµ¬ë§¤í›„ê¸° + ì¸ì¦ìƒ· ì—…ë¡œë“œ",
    "participation_deadline": "ë°©ì†¡ ì¢…ë£Œ í›„ 24ì‹œê°„",
    "winners_count": "10ëª…",
    "winner_criteria": "ì¶”ì²¨",
    "delivery_schedule": "ë‹¹ì²¨ ë°œí‘œ í›„ 2ì£¼ ì´ë‚´",
    "additional_info": "êµ¬ë§¤í™•ì • í›„ ì°¸ì—¬ ê°€ëŠ¥"
  }
]
```

### `notices` (ê³µì§€ì‚¬í•­ - vision-extracted)

```json
[
  {"source": "benefit", "content": "ì˜¤ëŠ˜ ë¼ì´ë¸Œ í•œì • ìµœëŒ€ 40% í• ì¸!"},
  {"source": "benefit", "content": "ì„ ì°©ìˆœ 100ëª… ì¶”ê°€ ì‚¬ì€í’ˆ ì¦ì •"}
]
```

### `precautions` (ìœ ì˜ì‚¬í•­ - vision-extracted, flattened)

```json
[
  {"category": "ë°°ì†¡", "content": "ì œì£¼/ë„ì„œì‚°ê°„ ì¶”ê°€ ë°°ì†¡ë¹„ 3,000ì›"},
  {"category": "êµí™˜/í™˜ë¶ˆ", "content": "7ì¼ ì´ë‚´ êµí™˜/í™˜ë¶ˆ ê°€ëŠ¥"}
]
```

### `announcements` (ê³µì§€ - API/notice pages)

Sources: Naver notice API, Amoremall notice page, SSG Gripcloud description

```json
[
  {
    "id": 12345,
    "title": "ðŸŽˆ ì•„ì´ì˜¤íŽ˜ ë¼ì´ë¸Œ(2/10) ðŸŽˆ ìµœì¢…ê³µì§€",
    "content": "ê³µì§€ ë‚´ìš© ì „ë¬¸...",
    "priority": "high",
    "timestamp": "2026-02-09T14:30:00+09:00",
    "notice_type": "FIXED_CHAT",
    "broadcast_id": "1843086"
  }
]
```

- `priority`: `"high"` (ìµœì¢…ê³µì§€) or `"medium"` (ì´ë²¤íŠ¸ ê³µì§€)
- `notice_type`: `"FIXED_CHAT"` (ê³ ì • ì±„íŒ…) or `"CHAT"` (ì¼ë°˜ ì±„íŒ…)
- Amoremall entries also have `source: "notice_page"`

### `host_chat` (í˜¸ìŠ¤íŠ¸ ì±„íŒ… - Naver replays/extras)

```json
[
  {
    "message": "ì§€ê¸ˆë¶€í„° ì¿ í° ì˜¤í”ˆí•©ë‹ˆë‹¤!",
    "priority": "high",
    "notice_type": "FIXED_CHAT",
    "timestamp_milli": 1234567
  }
]
```

- `timestamp_milli`: Milliseconds offset from broadcast start (NOT epoch)
- `priority`/`notice_type`: Same values as announcements

### `coupons_data` (ì¿ í° - Naver API)

```json
[
  {
    "title": "ë¼ì´ë¸Œ ì „ìš© 2,000ì› í• ì¸ì¿ í°",
    "benefit_type": "NEW",
    "benefit_unit": "FIX",
    "benefit_value": 2000,
    "min_order_amount": 20000,
    "max_discount_amount": null,
    "valid_start": "2026-02-09T00:00:00",
    "valid_end": "2026-02-16T23:59:59",
    "raw_data": { "..." }
  }
]
```

- `benefit_type`: `"NEW"` (new coupon) or `"NEWS"` (news coupon)
- `benefit_unit`: `"FIX"` (fixed amount) or `"RATE"` (percentage)

### `brand_data` (ë©€í‹° ë¸Œëžœë“œ ë¶„ë¥˜)

```json
{
  "brand_type": "single|collab|union",
  "title_brands": ["ì—ìŠ¤íŠ¸ë¼"],
  "product_brands": ["ì—ìŠ¤íŠ¸ë¼", "ë¼ë„¤ì¦ˆ", "ì•„ì´ì˜¤íŽ˜", "í•œìœ¨"],
  "all_brands": ["ë¼ë„¤ì¦ˆ", "ì•„ì´ì˜¤íŽ˜", "ì—ìŠ¤íŠ¸ë¼", "í•œìœ¨"],
  "extra_brands": ["ë¼ë„¤ì¦ˆ", "ì•„ì´ì˜¤íŽ˜", "í•œìœ¨"],
  "brand_ids": {
    "ì—ìŠ¤íŠ¸ë¼": "uuid-1",
    "ë¼ë„¤ì¦ˆ": "uuid-2",
    "ì•„ì´ì˜¤íŽ˜": "uuid-3",
    "í•œìœ¨": "uuid-4"
  }
}
```

### `comments_data` (í†µí•© ëŒ“ê¸€ - Naver livebridge)

```json
{
  "comments": [
    {
      "source": "pre_broadcast|post_broadcast",
      "source_label": "ì‚¬ì „ëŒ“ê¸€|ì‹œì²­ìžëŒ“ê¸€",
      "source_platform": "naver_livebridge|naver_replay|ssg_qna",
      "comment_id": "123456",
      "message": "ë°°ì†¡ ì–¼ë§ˆë‚˜ ê±¸ë ¤ìš”?",
      "created_at": "2026-01-27T10:30:00",
      "comment_type": "member|question|answer|viewer",
      "reactions": {"likes": 0, "dislikes": 0},
      "reply_count": 0,
      "metadata": {}
    }
  ],
  "summary": {
    "total_count": 38,
    "by_source": {"pre_broadcast": 35, "post_broadcast": 3}
  },
  "source_filters": ["pre_broadcast", "post_broadcast"]
}
```

### `promotion_images` (í”„ë¡œëª¨ì…˜ ì´ë¯¸ì§€)

Mixed formats by platform:

```json
// Naver: plain URL array
["https://shop-phinf.pstatic.net/image1.png", "https://...image2.png"]

// Kakao: object array with type
[
  {"url": "https://img.kakao.com/cover.jpg", "type": "cover"},
  {"url": "https://img.kakao.com/thumb.jpg", "type": "thumbnail"}
]
```

### `qna_data` (Q&A - SSG, Kakao)

> **Status**: Currently unused in production (0 rows). Intended for SSG/Kakao Q&A data.

```json
{
  "qna_items": [
    {
      "question_id": "q123",
      "question": "ë°°ì†¡ ì–¼ë§ˆë‚˜ ê±¸ë ¤ìš”?",
      "question_time": "2026-01-27T10:30:00",
      "answers": [
        {"answer": "ë³´í†µ 2-3ì¼ ì†Œìš”ë©ë‹ˆë‹¤", "answer_time": "2026-01-27T10:31:00"}
      ]
    }
  ],
  "summary": {"total_questions": 15, "total_answers": 12}
}
```

### Deprecated / Unused Columns

| Column | Status | Notes |
|--------|--------|-------|
| `purchase_benefits` | **Deprecated** | 0 rows in production. Use `benefits` (vision) or `benefits_data` (API) instead |
| `chat_summary` | **Unused** | 0 rows in production. Reserved for future chat statistics |

### Event Category Mapping (Vision â†’ DB)

| Vision Output (UPPERCASE) | Database (lowercase) |
|---------------------------|----------------------|
| PURCHASE_PROOF | purchase_verification |
| PURCHASE_KING | purchase_king |
| CHAT_KING | chat_king |
| PHOTO_REVIEW | photo_review |
| FIRST_COME | first_come |
| RAFFLE | raffle |
| SHARE | share |
| OTHER | other |

### Status Mapping

| Platform | Original | DB status |
|----------|----------|-----------|
| Naver | BLOCK | ended |
| Naver | END | ended |
| Naver | LIVE | live |
| Naver | READY | scheduled |
| Naver | (replay URL) | replay |
| SSG | (VOD crawler) | replay |
| SSG | (schedule crawler) | scheduled |
| Kakao | ON_AIR | live |
| Kakao | END | replay |
| Kakao | SCHEDULED | scheduled |

### ID Generation Strategy

| Platform | ID Source | Range |
|----------|-----------|-------|
| Naver | Native numeric ID | Direct use |
| SSG | MD5 hash of ch_xxxxx | 100M - 2.1B |
| Kakao | Native numeric ID | Direct use |
| 11ST | Native numeric ID | Direct use |
| CJ OnStyle | Native numeric ID | 0 - 10M |

## Research Workflow (New Platforms)

### Phase 1: Platform Investigation

1. **Explore platform structure**
   - Live broadcast listing page
   - VOD/replay listing page
   - Individual broadcast detail page
   - URL patterns

2. **Analyze data sources** (DevTools Network tab)
   - API endpoints (JSON) - easiest
   - HTML DOM structure - medium
   - Images/banners needing Vision - hardest

3. **Document in** `crawler/platforms/[platform]/investigation/`

### Phase 2: Data Mapping

Map platform fields to schema:

```
Platform Response          â†’ broadcasts table
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
broadcast_id/vod_id        â†’ external_id
title/name                 â†’ title
thumbnail/image            â†’ stand_by_image
start_time                 â†’ broadcast_date
scheduled_time             â†’ expected_start_date
coupons[]                  â†’ coupons_data (JSONB)
benefits[]                 â†’ benefits (JSONB, vision) or benefits_data (JSONB, API)
events[]                   â†’ participation_events (JSONB)
notices[]                  â†’ announcements (JSONB, API) or notices (JSONB, vision)

Platform Response          â†’ broadcast_products table
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
item_id                    â†’ product_id
item_name                  â†’ name
brand                      â†’ brand_name
price                      â†’ original_price
sale_price                 â†’ discounted_price
discount_rate              â†’ discount_rate
stock                      â†’ stock
classification             â†’ product_classification ('live' or 'main')
```

### Phase 3: Build Crawler

#### Directory Structure
```
crawler/platforms/[platform]/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ crawler.py              # Main crawler
â”œâ”€â”€ config.py               # URLs, endpoints
â”œâ”€â”€ html_selectors.py       # CSS selectors (if HTML)
â”œâ”€â”€ prompts.py              # Vision prompts (if needed)
â”œâ”€â”€ [platform]_persistence/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ saver.py            # Platform-specific save interface
â”‚   â””â”€â”€ transformer.py      # Data transformation (extends BaseTransformer)
â””â”€â”€ investigation/          # Research scripts (gitignored)
```

#### Transformer Template
```python
from crawler.shared.persistence.base_transformer import BaseTransformer

class PlatformTransformer(BaseTransformer):
    PLATFORM_CODE = 'PLATFORM'

    def transform_broadcast(self, crawler_data: Dict) -> Dict:
        """Map crawler output to broadcasts table schema"""
        pass

    def transform_products(self, broadcast_id: int, products: List) -> List[Dict]:
        """Map products to broadcast_products table schema"""
        pass

    def transform_notices(self, notices: List) -> List[Dict]:
        """Map notices to JSONB format: [{"content": "..."}]"""
        pass

    def transform_chat_messages(self, messages: List) -> List[Dict]:
        """Map chat to unified format"""
        pass
```

### Phase 4: Test

```bash
cd /var/www/html/ai_cs/crawler
source cj/venv/bin/activate
python platforms/[platform]/investigation/test_crawler.py
```

## Data Transformer Architecture

Transformers convert crawler output to frontend-ready database format. Each platform has its own transformer that extends `BaseTransformer`.

### Transformer Location
```
crawler/
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ brand_utils.py               # Brand detection + multi-brand classification
â”‚   â””â”€â”€ persistence/
â”‚       â”œâ”€â”€ base_transformer.py      # Abstract base class (with build_qna_data, build_comments_data)
â”‚       â”œâ”€â”€ upserter.py              # Multi-platform upsert with multi-brand detection
â”‚       â”œâ”€â”€ saver.py                 # Unified save interface
â”‚       â”œâ”€â”€ client.py                # Supabase client (get_brand_by_name, get_platform_by_code)
â”‚       â””â”€â”€ validator.py             # Schema validation
â”‚
â”œâ”€â”€ cj/persistence/
â”‚   â”œâ”€â”€ transformer.py               # Naver transformer (reference implementation)
â”‚   â””â”€â”€ upserter.py                  # Naver-specific upserter with multi-brand
â”‚
â””â”€â”€ platforms/
    â”œâ”€â”€ ssg/ssg_persistence/
    â”‚   â”œâ”€â”€ saver.py                 # SSG save interface
    â”‚   â””â”€â”€ transformer.py           # SSG transformer
    â””â”€â”€ kakao/kakao_persistence/
        â”œâ”€â”€ saver.py                 # Kakao save interface
        â””â”€â”€ transformer.py           # Kakao transformer
```

## Reference Files

| File | Purpose |
|------|---------|
| `crawler/shared/brand_utils.py` | Brand loading, keyword matching, multi-brand detection |
| `crawler/shared/persistence/base_transformer.py` | Abstract transformer base class |
| `crawler/shared/persistence/upserter.py` | Shared multi-platform upserter (SSG, Kakao, etc.) |
| `crawler/cj/persistence/transformer.py` | Naver transformer (most complete reference) |
| `crawler/cj/persistence/upserter.py` | Naver upserter with multi-brand |
| `crawler/platforms/ssg/crawler.py` | SSG crawler (HTML + API + Vision) |
| `crawler/platforms/ssg/ssg_persistence/transformer.py` | SSG transformer |
| `crawler/platforms/kakao/crawler.py` | Kakao crawler (API-based) |
| `docs/ai/design/feature-multi-platform-crawler-architecture.md` | Architecture design |

## Example Usage

```
User: /crawl-korea-live naver

Steps:
1. Research Naver Shopping Live structure
2. Find APIs for broadcast list, products
3. Identify what needs Vision extraction
4. Map to broadcasts + broadcast_products schema
5. Build crawler with persistence layer
6. Test and validate
```
